%!TEX root = ../main.tex

\chapter{Introduction\label{chap:introduction}}

This semestral project is focused on measuring the influence of distance and frequency of modulated
light source on event based camera response. Knowing the parameters of the light source, the camera with the lens
and how the response behaves, it is possible to predict the distance of the light source from the camera. This can be
used to improve the precision of localization algorithms, that are used in \ac{UAV} swarming applications. 

Event based cameras, in contrast to traditional frame based cameras, do not capture still frames but rather
provide an asynchronous and idependent stream of intensity changes on individual pixels.
Each pixel memorizes the last intensity value and sends an event when the intensity changes above a certain threshold.
%\cite{gallego2020event}
Event cameras circumvent many common issues found in traditional frame-based cameras, such as motion blur caused
by fast-moving objects. They offer significant advantages, including a high dynamic range, low latency,
and energy efficiency.
This makes them perfect fot the application of agile robotics,
where the fast response time is crucial (especially in UAV swarming situations). With their sub-millisecond response time,
event cameras can provide a significant advantage over traditional cameras in these applications.
However, they also come with some drawbacks, such as the need for a different approach to
data processing and the higher cost of the camera units themselves. \cite{gallego2020event}

The event data stream is represented by tuples of $(x, y, p, t)$, where $(x, y)$ are the pixel coordinates, $t$ is the time of
intensity change and $p$ is the polarity of the change - the increase or decrease of light intensity. Images can be
reconstructed from the event stream by integrating the events over time, doing so makes the usage of normal vision 
algorithms is possible, but is also goes against the main advantage of the event cameras - the low latency.

\section{Related Works}

The unique capabilities of event-based cameras have been extensively explored in various applications and studies.
A comprehensive survey by Gallego et al. highlights the advantages of event cameras in
applications requiring high-speed motion analysis, wide dynamic range, and low power consumption. \cite{gallego2020event}
%The survey also addresses the challenges of processing the asynchronous and sparse event data these cameras generate, 
%emphasizing the need for novel algorithms tailored to this unconventional sensing paradigm.

The potential of event cameras in vehicular \ac{VLC} has been demonstrated by Shen et al.
Their study introduced a framework utilizing a \ac{DVS} to mitigate the limitations of
traditional photodiode- and frame-based camera systems, such as limited throughput and vulnerability to ambient
interference. By leveraging the event-driven nature of DVS, the proposed system achieved significant improvements
in data transmission range and robustness. \cite{shen2019vehicular}

% In addition to VLC, event-based cameras have been successfully integrated into localization systems,
% yielding promising results for indoor positioning. Jung et al. introduced an indoor positioning method based on the
% \ac{RSSR} using LEDs for location estimation. This approach leverages the relative ratio of optical power detected by
% photodiodes from multiple LED sources, achieving precise positioning with a mean error of 3.24 cm. By incorporating
% time-division multiplexing, the method ensures robust identification of individual LEDs, enabling efficient localization.
% This system highlights potential applications in autonomous navigation and UAV swarming, benefiting from the method's high
% accuracy and low additional infrastructure requirements \cite{jung2014rssr}.

Event-based cameras have been integrated into localization systems with promising indoor positioning results.
Jung et al. demonstrated an \ac{RSSR}-based method using LEDs and time-division multiplexing, achieving precise positioning
with a mean error of 3.24 cm and minimal infrastructure requirements, suitable for applications like autonomous navigation
and UAV swarming \cite{jung2014rssr}.