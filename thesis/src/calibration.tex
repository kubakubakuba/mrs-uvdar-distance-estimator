%!TEX root = ../main.tex

\chapter{Camera calibration\label{chap:calib}}

As for the correct representation of distances and angles in the received data from the camera, a camera calibration needs to be performed.
The lens used when obtaining the data is a 2.5mm, f/1.6 fish-eye lens, with FOV of more than 90 degrees, which can be seen in \reffig{fig:fisheye_lens}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{./fig/photos/lens.jpeg}
  \caption{2.5mm f/1.6 fish eye lens.}
  \label{fig:fisheye_lens}
\end{figure}
In this work a calibration method proposed by Scaramuzza et al. \cite{scaramuzzacalibration} is used, which is based on the pinhole camera model, and is
used to calibrate fish eye lenses. The implementation used for the calibration is a python library py-OCamCalib \footnote{py-OCamCalib is available at \url{https://github.com/jakarto3d/py-OCamCalib}}.%TODO: maybe cite the github repo
For calibration purposes, a calibration chessboard pattern is usually used, which offers high contrast between squares (and thus easy detections of square corners) and
a known square size. Multiple images are usually taken, at various rotations and distances, to obtain good calibration results.
In our calibration procedure, we use a 5x7 lattice of UV LEDs, which are spaced 50 mm apart from each other. With this pattern, events are generated at the center of the
LEDs, and can be detected by any kind of blob detector. The LED lattice is used, so the event camera can easily detect the events from the bright LEDs, as opposed to
the light being reflected from the chessboard pattern (which does not produce any light on its own).
The calibration lattice can be seen in \reffig{fig:lattice}.

\begin{figure}[H]
	\centering
	\subfloat[Calibration lattice] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/lattice.jpeg}
	  \label{fig:lattice_1}
	}
	\subfloat[Image from the event camera of the calibration lattice] {
	  \includegraphics[width=0.5\textwidth]{./fig/photos/lattice_evs.png}
	  \label{fig:lattice_2}
	}
	\caption{
		Calibration lattice of 5x7 UV LEDs on \reffig{fig:lattice_1} and the events being produced when placed in front of the camera at \reffig{fig:lattice_2},
		with typical fish-eye lens distortion.
  }
	\label{fig:lattice}
\end{figure}
The downside of using an LED lattice instead of a chessboard pattern, is that at a very high FOV, the distortion of the fish eye lens sometimes does not allow
reliable detection of the exact blob centers (or which events correspond to which LED), as can be seen at \reffig{fig:calibration_pattern_distorted}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{./fig/photos/lattice_280.png}
  \caption{Calibration pattern with 5x7 lattice of UV LEDs, taken with a lens with FOV of 280 degrees.}
  \label{fig:calibration_pattern_distorted}
\end{figure}

As we are not using a regular image of a chessboard pattern, rather a recording of accumulated events, some data preprocessing has to be done beforehand.
For the purpose of this a simple Python app has been written. An event raw recording is loaded, and events are accumulated over a select period of time.
The accumulated events are then saved to a grayscale image, where each pixel corresponds to the number of events that occured at that
pixel \footnote{\url{https://github.com/kubakubakuba/metavision-pyocamcalib/blob/main/generate_frames.py}}. The image is then normalized,
and the LED centers are detected using \texttt{findContours} function from OpenCV \footnote{\url{https://github.com/kubakubakuba/metavision-pyocamcalib/blob/main/detect_blob_centers.py}}.
The detected centers can be then manually labeled (with optional corrections) row-wise, so the calibration script can then perform the calibration.
After this, the regular calibration script from py-OCamCalib can be used. The labeling of the LED centers can be seen in \reffig{fig:calibration_pattern_labeled}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{./fig/photos/lattice_blobs.png}
  \caption{Calibration pattern with 5x7 lattice of UV LEDs, with the centers of the LEDs being labeled.}
  \label{fig:calibration_pattern_labeled}
\end{figure}

After the calibration we are able to map the 3D world coordinates to the 2D image plane. For this, the calibration method needs to obtain the extrinsic
and intrinsic parameters of the camera, where the extrinsic parameters are the rotation and translation of the camera with respect to the world frame, and can be expressed
by equation \ref{eq:extrinsic}.
\begin{equation}
	\begin{bmatrix}
		X_{camera} \\
		Y_{camera} \\
		Z_{camera}
	\end{bmatrix}
	= R 
  \begin{bmatrix}
	X_{world} \\
	Y_{world} \\
	Z_{world}
  \end{bmatrix}
  + T
  \label{eq:extrinsic}
\end{equation}
which is an affine transformation that uses a rotation matrix $R$ and a translation vector $T$. The origin of the camera's coordinate system is at the optical center,
that is at the intersection of the optical axis from the center of the image with the image plane. This can be represented by \reffig{fig:camera_extrinsic}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{./fig/tikz/extrinsic.pdf}
  \caption{Extrinsic parameters of the camera.}
  \label{fig:camera_extrinsic}
\end{figure}

Next step in the camera calibration is to fit an encompassing ellipse to the received data. The purpose of this is to see the fish eye lens distortion center, as well
as the stretching of the image in both the x and y axis. This process can be seen in \reffig{fig:ellipse}. This ellipse is then used to calculate the point mapping functions, which allows for tranformation of points from
the image plane to their respective 3D coordinates.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{./fig/tikz/ellipse.pdf}
	\caption{}
	\label{fig:ellipse}
  \end{figure}

The intrinsic parameters of the camera specify the image format itself (influenced by the focal length, sensor size and optical center). As there are many mapping
functions that can be used while modelling the lens (their precision is limited by the manufacturing process), Scaramuzza et al. \cite{scaramuzzacalibration} proposed
to fit a polynomial to find the optimal model for the lens calibration. The mapping functions are shown in \reffig{fig:mapping_functions}.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{./fig/tikz/mapping.pdf}
  \caption{Fisheye mapping functions, $f$ is a parameter (focal length).}
  \label{fig:mapping_functions}
\end{figure}

After we fit a polynomial, we can map image points into their corresponding 3D vectors by an equation \ref{eq:intrinsic}.
\begin{equation}
	\lambda \cdot \alpha \cdot
	\begin{bmatrix}
		u' \\
		v' \\
		a_0 + a_1 \rho + \dots + a_{N} \rho^{N} 
	\end{bmatrix}
	= \mathbf{P} \cdot \mathbf{X_w} = %TODO: DOUBLE CHECK X_w is really the world point 
	\begin{bmatrix}
		X_c \\
		Y_c \\
		Z_c
	\end{bmatrix}
	\label{eq:intrinsic}
\end{equation}
where:

$\lambda$ is a scalar factor

$\alpha$ is a scaling factor that we obtain from \reffig{fig:ellipse} by stretching the ellipse back to a circle and performing
the affine transformation, $\alpha, \lambda > 0$ 

$\rho$ is the Euclidean distance of a point from the center $\rho = \sqrt{u^2 + v^2}$

$a_0, \dots a_N$ are the coefficients of the polynomial, and $\mathbf{P}$ is 
the perspective projection matrix.

We can also write another relation between a point $\textbf{m}' = \begin{bmatrix} u' & v' \end{bmatrix}^{T}$ on the image plane
and its corresponding point on the sensor plane $\textbf{m} = \begin{bmatrix} u & v \end{bmatrix}^{T}$. The coordinates of 
$\textbf{m}'$ have the origin at the top left corner of the image, whereas the coordinates of $\textbf{m}$ have origin at the image center.
This can be represented by an affine transformation $\textbf{m}' = \textbf{Am} + \textbf{O}_c$, on \ref{eq:affine} \cite{URBAN201572}.

\begin{equation}
	\begin{bmatrix}
		u' \\
		v'
	\end{bmatrix}
	= 
	\begin{bmatrix}
		c & d \\
		e & 1
	\end{bmatrix}
	\begin{bmatrix}
		u \\
		v
	\end{bmatrix}
	+
	\begin{bmatrix}
		o_u \\
		o_v
	\end{bmatrix}
\label{eq:affine}
\end{equation}

where the matrix A accounts for lens-sensor misalignment, and the vector $\textbf{O}_c$ captures the relation with the center of the distortion. 

\section{Calibration results}

The camera calibration was performed on a series of images, where the calibration lattice was placed as various angles and distances from the camera.
For ideal calibration results, the lattice should be placed in all visible parts of the image, as the distortion of the fish eye is more pronounced at the edges
of the visible area. The calibration was performed on a polynomial of degree 4, more would lead to overfitting and is not necessary.
The calibration results can be seen in \reffig{fig:calib_r}.

\begin{figure}[H]
	\centering
	\subfloat[Calibrated lens model function] {
	  \includegraphics[width=0.5\textwidth]{./fig/pgfplot/evk4_projection.pdf}
	  \label{fig:calib_r_1}
	}
	\subfloat[Calibration images mean reprojection errors] {
	  \includegraphics[width=0.5\textwidth]{./fig/pgfplot/evk4_reprojection_error.pdf}
	  \label{fig:calib_r_2}
	}
	\caption{
		Calibration results with the calibrated lens model function highlighted in red in \reffig{fig:calib_r_1} and the calibration images mean reprojection errors on \reffig{fig:calib_r_2}.
  }
	\label{fig:calib_r}
\end{figure}

We can now fit the encompassing ellipse to the data, by minimizing the number of events that are out of the ellipse (minimizing the ellipse size while keeping the
captured events inside). The ellipse can be seen in \reffig{fig:ellipse_fit}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{./fig/svg/ellipse_fit.pdf}
	\caption{Fitted ellipse to the calibration data, with major axis $a = 652$, and minor axis $b = 650$.}
	\label{fig:ellipse_fit}
\end{figure}

Finally, to visualize the calibration results, we map every point from the image plane using the \texttt{cam2world}\footnote{\url{https://github.com/jakarto3d/py-OCamCalib/blob/main/src/pyocamcalib/modelling/camera.py}}
function from py-OCamCalib, which takes a 2D image point, and returns
the corresponding 3D optical ray on the camera's unit sphere. For each point we calculate its angle from the optical axis (a vector $\mathbf{v} = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix}^{T}$),
and mask out the visible area with the ellipse fitted in \reffig{fig:ellipse_fit}. We can see the results in \reffig{fig:calibration_viz}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{./fig/pgfplot/evk4_viz.pdf}
	\caption{Angle from optical axis visualization, with the maximum angle of 93.47 degrees.}
	\label{fig:calibration_viz}
\end{figure}

We can also apply a perspective conversion to the whole image, which now represents distances and angles correctly. We can notice that by looking
at the calibration lattice at \reffig{fig:calib_c}, which now looks like a grid of points.

\begin{figure}[H]
	\centering
	\subfloat[Uncalibrated image.] {
	  \includegraphics[width=0.5\textwidth]{./fig/pgfplot/image_uncalib.pdf}
	  \label{fig:calib_c_1}
	}
	\subfloat[Calibrated image.] {
	  \includegraphics[width=0.5\textwidth]{./fig/pgfplot/image_calib.pdf}
	  \label{fig:calib_c_2}
	}
	\caption{
		Two photos of the calibration lattice, one uncalibrated on \reffig{fig:calib_c_1} and the one calibrated at \reffig{fig:calib_c_2}, which does not
		exhibit any distortion.
  }
	\label{fig:calib_c}
\end{figure}